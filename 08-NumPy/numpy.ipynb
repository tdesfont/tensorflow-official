{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfRdquslKbO3"
   },
   "source": [
    "# Load NumPy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0tqX8qkXZEj"
   },
   "source": [
    "This tutorial provides an example of loading data from NumPy arrays into a `tf.data.Dataset`.\n",
    "\n",
    "This example loads the MNIST dataset from a `.npz` file. However, the source of the NumPy arrays is not important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ze5IBx9clLB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6J3JzK5NxQ6"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    " \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0yWiN8-cpDb"
   },
   "source": [
    "### Load from `.npz` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AggregationMethod',\n",
       " 'Assert',\n",
       " 'CriticalSection',\n",
       " 'DType',\n",
       " 'DeviceSpec',\n",
       " 'GradientTape',\n",
       " 'Graph',\n",
       " 'IndexedSlices',\n",
       " 'IndexedSlicesSpec',\n",
       " 'Module',\n",
       " 'Operation',\n",
       " 'OptionalSpec',\n",
       " 'RaggedTensor',\n",
       " 'RaggedTensorSpec',\n",
       " 'RegisterGradient',\n",
       " 'SparseTensor',\n",
       " 'SparseTensorSpec',\n",
       " 'Tensor',\n",
       " 'TensorArray',\n",
       " 'TensorArraySpec',\n",
       " 'TensorShape',\n",
       " 'TensorSpec',\n",
       " 'TypeSpec',\n",
       " 'UnconnectedGradients',\n",
       " 'Variable',\n",
       " 'VariableAggregation',\n",
       " 'VariableSynchronization',\n",
       " '_LazyLoader',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__compiler_version__',\n",
       " '__cxx11_abi_flag__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__git_version__',\n",
       " '__loader__',\n",
       " '__monolithic_build__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_absolute_import',\n",
       " '_api',\n",
       " '_division',\n",
       " '_forward_module',\n",
       " '_importlib',\n",
       " '_m',\n",
       " '_print_function',\n",
       " '_root_estimator',\n",
       " '_sys',\n",
       " '_top_level_modules',\n",
       " '_types',\n",
       " 'abs',\n",
       " 'acos',\n",
       " 'acosh',\n",
       " 'add',\n",
       " 'add_n',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_dtype',\n",
       " 'as_string',\n",
       " 'asin',\n",
       " 'asinh',\n",
       " 'assert_equal',\n",
       " 'assert_greater',\n",
       " 'assert_less',\n",
       " 'assert_rank',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atanh',\n",
       " 'audio',\n",
       " 'autograph',\n",
       " 'batch_to_space',\n",
       " 'bfloat16',\n",
       " 'bitcast',\n",
       " 'bitwise',\n",
       " 'bool',\n",
       " 'boolean_mask',\n",
       " 'broadcast_dynamic_shape',\n",
       " 'broadcast_static_shape',\n",
       " 'broadcast_to',\n",
       " 'case',\n",
       " 'cast',\n",
       " 'clip_by_global_norm',\n",
       " 'clip_by_norm',\n",
       " 'clip_by_value',\n",
       " 'compat',\n",
       " 'complex',\n",
       " 'complex128',\n",
       " 'complex64',\n",
       " 'concat',\n",
       " 'cond',\n",
       " 'config',\n",
       " 'constant',\n",
       " 'constant_initializer',\n",
       " 'control_dependencies',\n",
       " 'convert_to_tensor',\n",
       " 'cos',\n",
       " 'cosh',\n",
       " 'cumsum',\n",
       " 'custom_gradient',\n",
       " 'data',\n",
       " 'debugging',\n",
       " 'device',\n",
       " 'distribute',\n",
       " 'divide',\n",
       " 'double',\n",
       " 'dtypes',\n",
       " 'dynamic_partition',\n",
       " 'dynamic_stitch',\n",
       " 'edit_distance',\n",
       " 'einsum',\n",
       " 'ensure_shape',\n",
       " 'equal',\n",
       " 'errors',\n",
       " 'estimator',\n",
       " 'examples',\n",
       " 'executing_eagerly',\n",
       " 'exp',\n",
       " 'expand_dims',\n",
       " 'experimental',\n",
       " 'extract_volume_patches',\n",
       " 'eye',\n",
       " 'feature_column',\n",
       " 'fill',\n",
       " 'fingerprint',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'floor',\n",
       " 'foldl',\n",
       " 'foldr',\n",
       " 'function',\n",
       " 'gather',\n",
       " 'gather_nd',\n",
       " 'get_logger',\n",
       " 'get_static_value',\n",
       " 'grad_pass_through',\n",
       " 'gradients',\n",
       " 'graph_util',\n",
       " 'greater',\n",
       " 'greater_equal',\n",
       " 'group',\n",
       " 'guarantee_const',\n",
       " 'half',\n",
       " 'hessians',\n",
       " 'histogram_fixed_width',\n",
       " 'histogram_fixed_width_bins',\n",
       " 'identity',\n",
       " 'identity_n',\n",
       " 'image',\n",
       " 'import_graph_def',\n",
       " 'init_scope',\n",
       " 'initializers',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'io',\n",
       " 'is_tensor',\n",
       " 'keras',\n",
       " 'less',\n",
       " 'less_equal',\n",
       " 'linalg',\n",
       " 'linspace',\n",
       " 'lite',\n",
       " 'load_library',\n",
       " 'load_op_library',\n",
       " 'logical_and',\n",
       " 'logical_not',\n",
       " 'logical_or',\n",
       " 'lookup',\n",
       " 'losses',\n",
       " 'make_ndarray',\n",
       " 'make_tensor_proto',\n",
       " 'map_fn',\n",
       " 'math',\n",
       " 'matmul',\n",
       " 'matrix_square_root',\n",
       " 'maximum',\n",
       " 'meshgrid',\n",
       " 'metrics',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'name_scope',\n",
       " 'negative',\n",
       " 'nest',\n",
       " 'newaxis',\n",
       " 'nn',\n",
       " 'no_gradient',\n",
       " 'no_op',\n",
       " 'nondifferentiable_batch_function',\n",
       " 'norm',\n",
       " 'not_equal',\n",
       " 'numpy_function',\n",
       " 'one_hot',\n",
       " 'ones',\n",
       " 'ones_initializer',\n",
       " 'ones_like',\n",
       " 'optimizers',\n",
       " 'pad',\n",
       " 'parallel_stack',\n",
       " 'pow',\n",
       " 'print',\n",
       " 'py_function',\n",
       " 'qint16',\n",
       " 'qint32',\n",
       " 'qint8',\n",
       " 'quantization',\n",
       " 'queue',\n",
       " 'quint16',\n",
       " 'quint8',\n",
       " 'ragged',\n",
       " 'random',\n",
       " 'random_normal_initializer',\n",
       " 'random_uniform_initializer',\n",
       " 'range',\n",
       " 'rank',\n",
       " 'raw_ops',\n",
       " 'realdiv',\n",
       " 'recompute_grad',\n",
       " 'reduce_all',\n",
       " 'reduce_any',\n",
       " 'reduce_logsumexp',\n",
       " 'reduce_max',\n",
       " 'reduce_mean',\n",
       " 'reduce_min',\n",
       " 'reduce_prod',\n",
       " 'reduce_sum',\n",
       " 'register_tensor_conversion_function',\n",
       " 'required_space_to_batch_paddings',\n",
       " 'reshape',\n",
       " 'resource',\n",
       " 'reverse',\n",
       " 'reverse_sequence',\n",
       " 'roll',\n",
       " 'round',\n",
       " 'saturate_cast',\n",
       " 'saved_model',\n",
       " 'scalar_mul',\n",
       " 'scan',\n",
       " 'scatter_nd',\n",
       " 'searchsorted',\n",
       " 'sequence_mask',\n",
       " 'sets',\n",
       " 'shape',\n",
       " 'shape_n',\n",
       " 'sigmoid',\n",
       " 'sign',\n",
       " 'signal',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'slice',\n",
       " 'sort',\n",
       " 'space_to_batch',\n",
       " 'space_to_batch_nd',\n",
       " 'sparse',\n",
       " 'split',\n",
       " 'sqrt',\n",
       " 'square',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'stop_gradient',\n",
       " 'strided_slice',\n",
       " 'string',\n",
       " 'strings',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'switch_case',\n",
       " 'sysconfig',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'tensor_scatter_nd_add',\n",
       " 'tensor_scatter_nd_sub',\n",
       " 'tensor_scatter_nd_update',\n",
       " 'tensordot',\n",
       " 'test',\n",
       " 'tile',\n",
       " 'timestamp',\n",
       " 'tpu',\n",
       " 'train',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncatediv',\n",
       " 'truncatemod',\n",
       " 'tuple',\n",
       " 'uint16',\n",
       " 'uint32',\n",
       " 'uint64',\n",
       " 'uint8',\n",
       " 'unique',\n",
       " 'unique_with_counts',\n",
       " 'unravel_index',\n",
       " 'unstack',\n",
       " 'variable_creator_scope',\n",
       " 'variant',\n",
       " 'vectorized_map',\n",
       " 'version',\n",
       " 'where',\n",
       " 'while_loop',\n",
       " 'xla',\n",
       " 'zeros',\n",
       " 'zeros_initializer',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Input',\n",
       " 'Model',\n",
       " 'Sequential',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_sys',\n",
       " 'activations',\n",
       " 'applications',\n",
       " 'backend',\n",
       " 'callbacks',\n",
       " 'constraints',\n",
       " 'datasets',\n",
       " 'estimator',\n",
       " 'experimental',\n",
       " 'initializers',\n",
       " 'layers',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'mixed_precision',\n",
       " 'models',\n",
       " 'optimizers',\n",
       " 'preprocessing',\n",
       " 'regularizers',\n",
       " 'utils',\n",
       " 'wrappers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GLHNrFM6RWoM"
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
    "\n",
    "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
    "with np.load(path) as data:\n",
    "  train_examples = data['x_train']\n",
    "  train_labels = data['y_train']\n",
    "  test_examples = data['x_test']\n",
    "  test_labels = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCeCkvrDgCMM"
   },
   "source": [
    "## Load NumPy arrays with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tslB0tJPgB-2"
   },
   "source": [
    "Assuming you have an array of examples and a corresponding array of labels, pass the two arrays as a tuple into `tf.data.Dataset.from_tensor_slices` to create a `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset',\n",
       " 'DatasetSpec',\n",
       " 'FixedLengthRecordDataset',\n",
       " 'Options',\n",
       " 'TFRecordDataset',\n",
       " 'TextLineDataset',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'experimental']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Hello')\n",
    "plt.imshow(train_examples[0], cmap='gray')\n",
    "plt.xlabel('A number in MNIST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Hello')\n",
    "plt.imshow(test_examples[0], cmap='gray')\n",
    "plt.xlabel('A number in MNIST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DatasetV2 in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "class DatasetV2(tensorflow.python.training.tracking.base.Trackable, tensorflow.python.framework.composite_tensor.CompositeTensor)\n",
      " |  Represents a potentially large set of elements.\n",
      " |  \n",
      " |  A `Dataset` can be used to represent an input pipeline as a\n",
      " |  collection of elements and a \"logical plan\" of transformations that act on\n",
      " |  those elements.\n",
      " |  \n",
      " |  A dataset contains elements that each have the same (nested) structure and the\n",
      " |  individual components of the structure can be of any type representable by\n",
      " |  `tf.TypeSpec`, including `tf.Tensor`, `tf.data.Dataset`, `tf.SparseTensor`,\n",
      " |  `tf.RaggedTensor`, or `tf.TensorArray`.\n",
      " |  \n",
      " |  Example elements:\n",
      " |  ```python\n",
      " |  # Integer element\n",
      " |  a = 1\n",
      " |  # Float element\n",
      " |  b = 2.0\n",
      " |  # Tuple element with 2 components\n",
      " |  c = (1, 2)\n",
      " |  # Dict element with 3 components\n",
      " |  d = {\"a\": (2, 2), \"b\": 3}\n",
      " |  # Element containing a dataset\n",
      " |  e = tf.data.Dataset.from_element(10)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DatasetV2\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.framework.composite_tensor.CompositeTensor\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, variant_tensor)\n",
      " |      Creates a DatasetV2 object.\n",
      " |      \n",
      " |      This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not\n",
      " |      take anything in its constructor whereas in the DatasetV2, we expect\n",
      " |      subclasses to create a variant_tensor and pass it in to the super() call.\n",
      " |      \n",
      " |      Args:\n",
      " |        variant_tensor: A DT_VARIANT tensor that represents the dataset.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Creates an `Iterator` for enumerating the elements of this dataset.\n",
      " |      \n",
      " |      The returned iterator implements the Python iterator protocol and therefore\n",
      " |      can only be used in eager mode.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Iterator` over the elements of this dataset.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If not inside of tf.function and not executing eagerly.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply(self, transformation_func)\n",
      " |      Applies a transformation function to this dataset.\n",
      " |      \n",
      " |      `apply` enables chaining of custom `Dataset` transformations, which are\n",
      " |      represented as functions that take one `Dataset` argument and return a\n",
      " |      transformed `Dataset`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```\n",
      " |      dataset = (dataset.map(lambda x: x ** 2)\n",
      " |                 .apply(group_by_window(key_func, reduce_func, window_size))\n",
      " |                 .map(lambda x: x ** 3))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        transformation_func: A function that takes one `Dataset` argument and\n",
      " |          returns a `Dataset`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` returned by applying `transformation_func` to this\n",
      " |            dataset.\n",
      " |  \n",
      " |  batch(self, batch_size, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into batches.\n",
      " |      \n",
      " |      The components of the resulting element will have an additional outer\n",
      " |      dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
      " |      element if `batch_size` does not divide the number of input elements `N`\n",
      " |      evenly and `drop_remainder` is `False`). If your program depends on the\n",
      " |      batches having the same outer dimension, you should set the `drop_remainder`\n",
      " |      argument to `True` to prevent the smaller batch from being produced.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  cache(self, filename='')\n",
      " |      Caches the elements in this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
      " |          directory on the filesystem to use for caching elements in this Dataset.\n",
      " |          If a filename is not provided, the dataset will be cached in memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  concatenate(self, dataset)\n",
      " |      Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      a = Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      b = Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
      " |      \n",
      " |      # The input dataset and dataset to be concatenated should have the same\n",
      " |      # nested structures and output types.\n",
      " |      # c = Dataset.range(8, 14).batch(2)  # ==> [ [8, 9], [10, 11], [12, 13] ]\n",
      " |      # d = Dataset.from_tensor_slices([14.0, 15.0, 16.0])\n",
      " |      # a.concatenate(c) and a.concatenate(d) would result in error.\n",
      " |      \n",
      " |      a.concatenate(b)  # ==> [ 1, 2, 3, 4, 5, 6, 7 ]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        dataset: `Dataset` to be concatenated.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  enumerate(self, start=0)\n",
      " |      Enumerates the elements of this dataset.\n",
      " |      \n",
      " |      It is similar to python's `enumerate`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      a = { 1, 2, 3 }\n",
      " |      b = { (7, 8), (9, 10) }\n",
      " |      \n",
      " |      # The nested structure of the `datasets` argument determines the\n",
      " |      # structure of elements in the resulting dataset.\n",
      " |      a.enumerate(start=5)) == { (5, 1), (6, 2), (7, 3) }\n",
      " |      b.enumerate() == { (0, (7, 8)), (1, (9, 10)) }\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
      " |          enumeration.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  filter(self, predicate)\n",
      " |      Filters this dataset according to `predicate`.\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
      " |      \n",
      " |      d = d.filter(lambda x: x < 3)  # ==> [1, 2]\n",
      " |      \n",
      " |      # `tf.math.equal(x, y)` is required for equality comparison\n",
      " |      def filter_fn(x):\n",
      " |        return tf.math.equal(x, 1)\n",
      " |      \n",
      " |      d = d.filter(filter_fn)  # ==> [1]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        predicate: A function mapping a dataset element to a boolean.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: The `Dataset` containing the elements of this dataset for which\n",
      " |            `predicate` is `True`.\n",
      " |  \n",
      " |  flat_map(self, map_func)\n",
      " |      Maps `map_func` across this dataset and flattens the result.\n",
      " |      \n",
      " |      Use `flat_map` if you want to make sure that the order of your dataset\n",
      " |      stays the same. For example, to flatten a dataset of batches into a\n",
      " |      dataset of their elements:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = Dataset.from_tensor_slices([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ])\n",
      " |      \n",
      " |      a.flat_map(lambda x: Dataset.from_tensor_slices(x + 1)) # ==>\n",
      " |      #  [ 2, 3, 4, 5, 6, 7, 8, 9, 10 ]\n",
      " |      ```\n",
      " |      \n",
      " |      `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
      " |      `flat_map` produces the same output as\n",
      " |      `tf.data.Dataset.interleave(cycle_length=1)`\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  interleave(self, map_func, cycle_length=-1, block_length=1, num_parallel_calls=None)\n",
      " |      Maps `map_func` across this dataset, and interleaves the results.\n",
      " |      \n",
      " |      For example, you can use `Dataset.interleave()` to process many input files\n",
      " |      concurrently:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Preprocess 4 files concurrently, and interleave blocks of 16 records from\n",
      " |      # each file.\n",
      " |      filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\", ...]\n",
      " |      dataset = (Dataset.from_tensor_slices(filenames)\n",
      " |                 .interleave(lambda x:\n",
      " |                     TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
      " |                     cycle_length=4, block_length=16))\n",
      " |      ```\n",
      " |      \n",
      " |      The `cycle_length` and `block_length` arguments control the order in which\n",
      " |      elements are produced. `cycle_length` controls the number of input elements\n",
      " |      that are processed concurrently. If you set `cycle_length` to 1, this\n",
      " |      transformation will handle one input element at a time, and will produce\n",
      " |      identical results to `tf.data.Dataset.flat_map`. In general,\n",
      " |      this transformation will apply `map_func` to `cycle_length` input elements,\n",
      " |      open iterators on the returned `Dataset` objects, and cycle through them\n",
      " |      producing `block_length` consecutive elements from each iterator, and\n",
      " |      consuming the next input element each time it reaches the end of an\n",
      " |      iterator.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      \n",
      " |      # NOTE: New lines indicate \"block\" boundaries.\n",
      " |      a.interleave(lambda x: Dataset.from_tensors(x).repeat(6),\n",
      " |                  cycle_length=2, block_length=4)  # ==> [1, 1, 1, 1,\n",
      " |                                                   #      2, 2, 2, 2,\n",
      " |                                                   #      1, 1,\n",
      " |                                                   #      2, 2,\n",
      " |                                                   #      3, 3, 3, 3,\n",
      " |                                                   #      4, 4, 4, 4,\n",
      " |                                                   #      3, 3,\n",
      " |                                                   #      4, 4,\n",
      " |                                                   #      5, 5, 5, 5,\n",
      " |                                                   #      5, 5]\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The order of elements yielded by this transformation is\n",
      " |      deterministic, as long as `map_func` is a pure function. If\n",
      " |      `map_func` contains any stateful operations, the order in which\n",
      " |      that state is accessed is undefined.\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to a dataset.\n",
      " |        cycle_length: (Optional.) The number of input elements that will be\n",
      " |          processed concurrently. If not specified, the value will be derived from\n",
      " |          the number of available CPU cores. If the `num_parallel_calls` argument\n",
      " |          is set to `tf.data.experimental.AUTOTUNE`, the `cycle_length` argument\n",
      " |          also identifies the maximum degree of parallelism.\n",
      " |        block_length: (Optional.) The number of consecutive elements to produce\n",
      " |          from each input element before cycling to another input element.\n",
      " |        num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
      " |          threadpool, which is used to fetch inputs from cycle elements\n",
      " |          asynchronously and in parallel. The default behavior is to fetch inputs\n",
      " |          from cycle elements synchronously with no parallelism. If the value\n",
      " |          `tf.data.experimental.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  map(self, map_func, num_parallel_calls=None)\n",
      " |      Maps `map_func` across the elements of this dataset.\n",
      " |      \n",
      " |      This transformation applies `map_func` to each element of this dataset, and\n",
      " |      returns a new dataset containing the transformed elements, in the same\n",
      " |      order as they appeared in the input.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
      " |      \n",
      " |      a.map(lambda x: x + 1)  # ==> [ 2, 3, 4, 5, 6 ]\n",
      " |      ```\n",
      " |      \n",
      " |      The input signature of `map_func` is determined by the structure of each\n",
      " |      element in this dataset. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following examples use `{ ... }` to represent the\n",
      " |      # contents of a dataset.\n",
      " |      # Each element is a `tf.Tensor` object.\n",
      " |      a = { 1, 2, 3, 4, 5 }\n",
      " |      # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
      " |      # shape and dtype.\n",
      " |      result = a.map(lambda x: ...)\n",
      " |      \n",
      " |      # Each element is a tuple containing two `tf.Tensor` objects.\n",
      " |      b = { (1, \"foo\"), (2, \"bar\"), (3, \"baz\") }\n",
      " |      # `map_func` takes two arguments of type `tf.Tensor`.\n",
      " |      result = b.map(lambda x_int, y_str: ...)\n",
      " |      \n",
      " |      # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
      " |      c = { {\"a\": 1, \"b\": \"foo\"}, {\"a\": 2, \"b\": \"bar\"}, {\"a\": 3, \"b\": \"baz\"} }\n",
      " |      # `map_func` takes a single argument of type `dict` with the same keys as\n",
      " |      # the elements.\n",
      " |      result = c.map(lambda d: ...)\n",
      " |      ```\n",
      " |      \n",
      " |      The value or values returned by `map_func` determine the structure of each\n",
      " |      element in the returned dataset.\n",
      " |      \n",
      " |      ```python\n",
      " |      # `map_func` returns a scalar `tf.Tensor` of type `tf.float32`.\n",
      " |      def f(...):\n",
      " |        return tf.constant(37.0)\n",
      " |      result = dataset.map(f)\n",
      " |      result.output_classes == tf.Tensor\n",
      " |      result.output_types == tf.float32\n",
      " |      result.output_shapes == []  # scalar\n",
      " |      \n",
      " |      # `map_func` returns two `tf.Tensor` objects.\n",
      " |      def g(...):\n",
      " |        return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
      " |      result = dataset.map(g)\n",
      " |      result.output_classes == (tf.Tensor, tf.Tensor)\n",
      " |      result.output_types == (tf.float32, tf.string)\n",
      " |      result.output_shapes == ([], [3])\n",
      " |      \n",
      " |      # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
      " |      # `tf.Tensor`.\n",
      " |      def h(...):\n",
      " |        return 37.0, [\"Foo\", \"Bar\", \"Baz\"], np.array([1.0, 2.0] dtype=np.float64)\n",
      " |      result = dataset.map(h)\n",
      " |      result.output_classes == (tf.Tensor, tf.Tensor, tf.Tensor)\n",
      " |      result.output_types == (tf.float32, tf.string, tf.float64)\n",
      " |      result.output_shapes == ([], [3], [2])\n",
      " |      \n",
      " |      # `map_func` can return nested structures.\n",
      " |      def i(...):\n",
      " |        return {\"a\": 37.0, \"b\": [42, 16]}, \"foo\"\n",
      " |      result.output_classes == ({\"a\": tf.Tensor, \"b\": tf.Tensor}, tf.Tensor)\n",
      " |      result.output_types == ({\"a\": tf.float32, \"b\": tf.int32}, tf.string)\n",
      " |      result.output_shapes == ({\"a\": [], \"b\": [2]}, [])\n",
      " |      ```\n",
      " |      \n",
      " |      `map_func` can accept as arguments and return any type of dataset element.\n",
      " |      \n",
      " |      Note that irrespective of the context in which `map_func` is defined (eager\n",
      " |      vs. graph), tf.data traces the function and executes it as a graph. To use\n",
      " |      Python code inside of the function you have two options:\n",
      " |      \n",
      " |      1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
      " |      computation. The downside of this approach is that AutoGraph can convert\n",
      " |      some but not all Python code.\n",
      " |      \n",
      " |      2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
      " |      will generally result in worse performance than 1). For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
      " |      \n",
      " |      # transform a string tensor to upper case string using a Python function\n",
      " |      def upper_case_fn(t: tf.Tensor) -> str:\n",
      " |          return t.numpy().decode('utf-8').upper()\n",
      " |      \n",
      " |      d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
      " |            inp=[x], Tout=tf.string))  # ==> [ \"HELLO\", \"WORLD\" ]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        map_func: A function mapping a dataset element to another dataset element.\n",
      " |        num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`,\n",
      " |          representing the number elements to process asynchronously in parallel.\n",
      " |          If not specified, elements will be processed sequentially. If the value\n",
      " |          `tf.data.experimental.AUTOTUNE` is used, then the number of parallel\n",
      " |          calls is set dynamically based on available CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  options(self)\n",
      " |      Returns the options for this dataset and its inputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.data.Options` object representing the dataset options.\n",
      " |  \n",
      " |  padded_batch(self, batch_size, padded_shapes, padding_values=None, drop_remainder=False)\n",
      " |      Combines consecutive elements of this dataset into padded batches.\n",
      " |      \n",
      " |      This transformation combines multiple consecutive elements of the input\n",
      " |      dataset into a single element.\n",
      " |      \n",
      " |      Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
      " |      have an additional outer dimension, which will be `batch_size` (or\n",
      " |      `N % batch_size` for the last element if `batch_size` does not divide the\n",
      " |      number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
      " |      your program depends on the batches having the same outer dimension, you\n",
      " |      should set the `drop_remainder` argument to `True` to prevent the smaller\n",
      " |      batch from being produced.\n",
      " |      \n",
      " |      Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
      " |      different shapes, and this transformation will pad each component to the\n",
      " |      respective shape in `padding_shapes`. The `padding_shapes` argument\n",
      " |      determines the resulting shape for each dimension of each component in an\n",
      " |      output element:\n",
      " |      \n",
      " |      * If the dimension is a constant (e.g. `tf.compat.v1.Dimension(37)`), the\n",
      " |      component\n",
      " |        will be padded out to that length in that dimension.\n",
      " |      * If the dimension is unknown (e.g. `tf.compat.v1.Dimension(None)`), the\n",
      " |      component\n",
      " |        will be padded out to the maximum length of all elements in that\n",
      " |        dimension.\n",
      " |      \n",
      " |      See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
      " |      elements that may have different shapes into a `tf.SparseTensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          consecutive elements of this dataset to combine in a single batch.\n",
      " |        padded_shapes: A nested structure of `tf.TensorShape` or `tf.int64` vector\n",
      " |          tensor-like objects representing the shape to which the respective\n",
      " |          component of each input element should be padded prior to batching. Any\n",
      " |          unknown dimensions (e.g. `tf.compat.v1.Dimension(None)` in a\n",
      " |          `tf.TensorShape` or `-1` in a tensor-like object) will be padded to the\n",
      " |          maximum size of that dimension in each batch.\n",
      " |        padding_values: (Optional.) A nested structure of scalar-shaped\n",
      " |          `tf.Tensor`, representing the padding values to use for the respective\n",
      " |          components.  Defaults are `0` for numeric types and the empty string for\n",
      " |          string types.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether the last batch should be dropped in the case it has fewer than\n",
      " |          `batch_size` elements; the default behavior is not to drop the smaller\n",
      " |          batch.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  prefetch(self, buffer_size)\n",
      " |      Creates a `Dataset` that prefetches elements from this dataset.\n",
      " |      \n",
      " |      Note: Like other `Dataset` methods, prefetch operates on the\n",
      " |      elements of the input dataset. It has no concept of examples vs. batches.\n",
      " |      `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
      " |      while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
      " |      (2 batches, of 20 examples each).\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
      " |          number of elements that will be buffered when prefetching.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  reduce(self, initial_state, reduce_func)\n",
      " |      Reduces the input dataset to a single element.\n",
      " |      \n",
      " |      The transformation calls `reduce_func` successively on every element of\n",
      " |      the input dataset until the dataset is exhausted, aggregating information in\n",
      " |      its internal state. The `initial_state` argument is used for the initial\n",
      " |      state and the final state is returned as the result.\n",
      " |      \n",
      " |      For example:\n",
      " |      - `tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1)`\n",
      " |        produces `5`\n",
      " |      - `tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y)`\n",
      " |        produces `10`\n",
      " |      \n",
      " |      Args:\n",
      " |        initial_state: An element representing the initial state of the\n",
      " |          transformation.\n",
      " |        reduce_func: A function that maps `(old_state, input_element)` to\n",
      " |          `new_state`. It must take two arguments and return a new element\n",
      " |          The structure of `new_state` must match the structure of\n",
      " |          `initial_state`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A dataset element corresponding to the final state of the transformation.\n",
      " |  \n",
      " |  repeat(self, count=None)\n",
      " |      Repeats this dataset `count` times.\n",
      " |      \n",
      " |      NOTE: If this dataset is a function of global state (e.g. a random number\n",
      " |      generator), then different repetitions may produce different elements.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          number of times the dataset should be repeated. The default behavior (if\n",
      " |          `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  shard(self, num_shards, index)\n",
      " |      Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
      " |      \n",
      " |      This dataset operator is very useful when running distributed training, as\n",
      " |      it allows each worker to read a unique subset.\n",
      " |      \n",
      " |      When reading a single input file, you can skip elements as follows:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = tf.data.TFRecordDataset(input_file)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Important caveats:\n",
      " |      \n",
      " |      - Be sure to shard before you use any randomizing operator (such as\n",
      " |        shuffle).\n",
      " |      - Generally it is best if the shard operator is used early in the dataset\n",
      " |        pipeline. For example, when reading from a set of TFRecord files, shard\n",
      " |        before converting the dataset to input samples. This avoids reading every\n",
      " |        file on every worker. The following is an example of an efficient\n",
      " |        sharding strategy within a complete pipeline:\n",
      " |      \n",
      " |      ```python\n",
      " |      d = Dataset.list_files(pattern)\n",
      " |      d = d.shard(num_workers, worker_index)\n",
      " |      d = d.repeat(num_epochs)\n",
      " |      d = d.shuffle(shuffle_buffer_size)\n",
      " |      d = d.interleave(tf.data.TFRecordDataset,\n",
      " |                       cycle_length=num_readers, block_length=1)\n",
      " |      d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          shards operating in parallel.\n",
      " |        index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
      " |          Note: error checking is done on a best-effort basis, and errors aren't\n",
      " |          guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
      " |          placeholder tensor bypasses the early checking, and will instead result\n",
      " |          in an error during a session.run call.)\n",
      " |  \n",
      " |  shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
      " |      Randomly shuffles the elements of this dataset.\n",
      " |      \n",
      " |      This dataset fills a buffer with `buffer_size` elements, then randomly\n",
      " |      samples elements from this buffer, replacing the selected elements with new\n",
      " |      elements. For perfect shuffling, a buffer size greater than or equal to the\n",
      " |      full size of the dataset is required.\n",
      " |      \n",
      " |      For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
      " |      set to 1,000, then `shuffle` will initially select a random element from\n",
      " |      only the first 1,000 elements in the buffer. Once an element is selected,\n",
      " |      its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
      " |      maintaining the 1,000 element buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |        buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements from this dataset from which the new dataset will sample.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.compat.v1.set_random_seed` for behavior.\n",
      " |        reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
      " |          that the dataset should be pseudorandomly reshuffled each time it is\n",
      " |          iterated over. (Defaults to `True`.)\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  skip(self, count)\n",
      " |      Creates a `Dataset` that skips `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be skipped to form the new dataset.\n",
      " |          If `count` is greater than the size of this dataset, the new dataset\n",
      " |          will contain no elements.  If `count` is -1, skips the entire dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  take(self, count)\n",
      " |      Creates a `Dataset` with at most `count` elements from this dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |        count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      " |          elements of this dataset that should be taken to form the new dataset.\n",
      " |          If `count` is -1, or if `count` is greater than the size of this\n",
      " |          dataset, the new dataset will contain all elements of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  unbatch(self)\n",
      " |      Splits elements of a dataset into multiple elements.\n",
      " |      \n",
      " |      For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
      " |      where `B` may vary for each input element, then for each element in the\n",
      " |      dataset, the unbatched dataset will contain `B` consecutive elements\n",
      " |      of shape `[a0, a1, ...]`.\n",
      " |      \n",
      " |      ```python\n",
      " |      # NOTE: The following example uses `{ ... }` to represent the contents\n",
      " |      # of a dataset.\n",
      " |      ds = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n",
      " |      \n",
      " |      ds.unbatch() == {'a', 'b', 'c', 'a', 'b', 'a', 'b', 'c', 'd'}\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Dataset` transformation function, which can be passed to\n",
      " |        `tf.data.Dataset.apply`.\n",
      " |  \n",
      " |  window(self, size, shift=None, stride=1, drop_remainder=False)\n",
      " |      Combines (nests of) input elements into a dataset of (nests of) windows.\n",
      " |      \n",
      " |      A \"window\" is a finite dataset of flat elements of size `size` (or possibly\n",
      " |      fewer if there are not enough input elements to fill the window and\n",
      " |      `drop_remainder` evaluates to false).\n",
      " |      \n",
      " |      The `stride` argument determines the stride of the input elements, and the\n",
      " |      `shift` argument determines the shift of the window.\n",
      " |      \n",
      " |      For example, letting {...} to represent a Dataset:\n",
      " |      \n",
      " |      - `tf.data.Dataset.range(7).window(2)` produces\n",
      " |        `{{0, 1}, {2, 3}, {4, 5}, {6}}`\n",
      " |      - `tf.data.Dataset.range(7).window(3, 2, 1, True)` produces\n",
      " |        `{{0, 1, 2}, {2, 3, 4}, {4, 5, 6}}`\n",
      " |      - `tf.data.Dataset.range(7).window(3, 1, 2, True)` produces\n",
      " |        `{{0, 2, 4}, {1, 3, 5}, {2, 4, 6}}`\n",
      " |      \n",
      " |      Note that when the `window` transformation is applied to a dataset of\n",
      " |      nested elements, it produces a dataset of nested windows.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      - `tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)`\n",
      " |        produces `{({0, 1}, {0, 1}), ({2, 3}, {2, 3})}`\n",
      " |      - `tf.data.Dataset.from_tensor_slices({\"a\": range(4)}).window(2)`\n",
      " |        produces `{{\"a\": {0, 1}}, {\"a\": {2, 3}}}`\n",
      " |      \n",
      " |      Args:\n",
      " |        size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
      " |          of the input dataset to combine into a window.\n",
      " |        shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          forward shift of the sliding window in each iteration. Defaults to\n",
      " |          `size`.\n",
      " |        stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
      " |          stride of the input elements in the sliding window.\n",
      " |        drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
      " |          whether a window should be dropped in case its size is smaller than\n",
      " |          `window_size`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` of (nests of) windows -- a finite datasets of flat\n",
      " |          elements created from the (nests of) input elements.\n",
      " |  \n",
      " |  with_options(self, options)\n",
      " |      Returns a new `tf.data.Dataset` with the given options set.\n",
      " |      \n",
      " |      The options are \"global\" in the sense they apply to the entire dataset.\n",
      " |      If options are set multiple times, they are merged as long as different\n",
      " |      options do not use different non-default values.\n",
      " |      \n",
      " |      Args:\n",
      " |        options: A `tf.data.Options` that identifies the options the use.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset` with the given options.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when an option is set more than once to a non-default value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_generator(generator, output_types, output_shapes=None, args=None)\n",
      " |      Creates a `Dataset` whose elements are generated by `generator`.\n",
      " |      \n",
      " |      The `generator` argument must be a callable object that returns\n",
      " |      an object that supports the `iter()` protocol (e.g. a generator function).\n",
      " |      The elements generated by `generator` must be compatible with the given\n",
      " |      `output_types` and (optional) `output_shapes` arguments.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      import itertools\n",
      " |      tf.compat.v1.enable_eager_execution()\n",
      " |      \n",
      " |      def gen():\n",
      " |        for i in itertools.count(1):\n",
      " |          yield (i, [1] * i)\n",
      " |      \n",
      " |      ds = tf.data.Dataset.from_generator(\n",
      " |          gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))\n",
      " |      \n",
      " |      for value in ds.take(2):\n",
      " |        print value\n",
      " |      # (1, array([1]))\n",
      " |      # (2, array([1, 1]))\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: The current implementation of `Dataset.from_generator()` uses\n",
      " |      `tf.numpy_function` and inherits the same constraints. In particular, it\n",
      " |      requires the `Dataset`- and `Iterator`-related operations to be placed\n",
      " |      on a device in the same process as the Python program that called\n",
      " |      `Dataset.from_generator()`. The body of `generator` will not be\n",
      " |      serialized in a `GraphDef`, and you should not use this method if you\n",
      " |      need to serialize your model and restore it in a different environment.\n",
      " |      \n",
      " |      NOTE: If `generator` depends on mutable global variables or other external\n",
      " |      state, be aware that the runtime may invoke `generator` multiple times\n",
      " |      (in order to support repeating the `Dataset`) and at any time\n",
      " |      between the call to `Dataset.from_generator()` and the production of the\n",
      " |      first element from the generator. Mutating global variables or external\n",
      " |      state can cause undefined behavior, and we recommend that you explicitly\n",
      " |      cache any external state in `generator` before calling\n",
      " |      `Dataset.from_generator()`.\n",
      " |      \n",
      " |      Args:\n",
      " |        generator: A callable object that returns an object that supports the\n",
      " |          `iter()` protocol. If `args` is not specified, `generator` must take no\n",
      " |          arguments; otherwise it must take as many arguments as there are values\n",
      " |          in `args`.\n",
      " |        output_types: A nested structure of `tf.DType` objects corresponding to\n",
      " |          each component of an element yielded by `generator`.\n",
      " |        output_shapes: (Optional.) A nested structure of `tf.TensorShape` objects\n",
      " |          corresponding to each component of an element yielded by `generator`.\n",
      " |        args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
      " |          and passed to `generator` as NumPy-array arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensor_slices(tensors)\n",
      " |      Creates a `Dataset` whose elements are slices of the given tensors.\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this guide](\n",
      " |      https://tensorflow.org/guide/datasets#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element, with each component having the same size in\n",
      " |          the 0th dimension.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  from_tensors(tensors)\n",
      " |      Creates a `Dataset` with a single element, comprising the given tensors.\n",
      " |      \n",
      " |      Note that if `tensors` contains a NumPy array, and eager execution is not\n",
      " |      enabled, the values will be embedded in the graph as one or more\n",
      " |      `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
      " |      memory and run into byte limits of graph serialization. If `tensors`\n",
      " |      contains one or more large NumPy arrays, consider the alternative described\n",
      " |      in [this\n",
      " |      guide](https://tensorflow.org/guide/datasets#consuming_numpy_arrays).\n",
      " |      \n",
      " |      Args:\n",
      " |        tensors: A dataset element.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  list_files(file_pattern, shuffle=None, seed=None)\n",
      " |      A dataset of all files matching one or more glob patterns.\n",
      " |      \n",
      " |      NOTE: The default behavior of this method is to return filenames in\n",
      " |      a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
      " |      to get results in a deterministic order.\n",
      " |      \n",
      " |      Example:\n",
      " |        If we had the following files on our filesystem:\n",
      " |          - /path/to/dir/a.txt\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |        If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
      " |        would produce:\n",
      " |          - /path/to/dir/b.py\n",
      " |          - /path/to/dir/c.py\n",
      " |      \n",
      " |      Args:\n",
      " |        file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
      " |          (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
      " |          pattern(s) that will be matched.\n",
      " |        shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
      " |          Defaults to `True`.\n",
      " |        seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
      " |          seed that will be used to create the distribution. See\n",
      " |          `tf.compat.v1.set_random_seed` for behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |       Dataset: A `Dataset` of strings corresponding to file names.\n",
      " |  \n",
      " |  range(*args)\n",
      " |      Creates a `Dataset` of a step-separated range of values.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      Dataset.range(5) == [0, 1, 2, 3, 4]\n",
      " |      Dataset.range(2, 5) == [2, 3, 4]\n",
      " |      Dataset.range(1, 5, 2) == [1, 3]\n",
      " |      Dataset.range(1, 5, -2) == []\n",
      " |      Dataset.range(5, 1) == []\n",
      " |      Dataset.range(5, 1, -2) == [5, 3]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: follows the same semantics as python's xrange.\n",
      " |          len(args) == 1 -> start = 0, stop = args[0], step = 1\n",
      " |          len(args) == 2 -> start = args[0], stop = args[1], step = 1\n",
      " |          len(args) == 3 -> start = args[0], stop = args[1, stop = args[2]\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `RangeDataset`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if len(args) == 0.\n",
      " |  \n",
      " |  zip(datasets)\n",
      " |      Creates a `Dataset` by zipping together the given datasets.\n",
      " |      \n",
      " |      This method has similar semantics to the built-in `zip()` function\n",
      " |      in Python, with the main difference being that the `datasets`\n",
      " |      argument can be an arbitrary nested structure of `Dataset` objects.\n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
      " |      b = Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
      " |      c = Dataset.range(7, 13).batch(2)  # ==> [ [7, 8], [9, 10], [11, 12] ]\n",
      " |      d = Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
      " |      \n",
      " |      # The nested structure of the `datasets` argument determines the\n",
      " |      # structure of elements in the resulting dataset.\n",
      " |      Dataset.zip((a, b))  # ==> [ (1, 4), (2, 5), (3, 6) ]\n",
      " |      Dataset.zip((b, a))  # ==> [ (4, 1), (5, 2), (6, 3) ]\n",
      " |      \n",
      " |      # The `datasets` argument may contain an arbitrary number of\n",
      " |      # datasets.\n",
      " |      Dataset.zip((a, b, c))  # ==> [ (1, 4, [7, 8]),\n",
      " |                              #       (2, 5, [9, 10]),\n",
      " |                              #       (3, 6, [11, 12]) ]\n",
      " |      \n",
      " |      # The number of elements in the resulting dataset is the same as\n",
      " |      # the size of the smallest dataset in `datasets`.\n",
      " |      Dataset.zip((a, d))  # ==> [ (1, 13), (2, 14) ]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        datasets: A nested structure of datasets.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dataset: A `Dataset`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  element_spec\n",
      " |      The type specification of an element of this dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A nested structure of `tf.TypeSpec` objects matching the structure of an\n",
      " |        element of this dataset and specifying the type of individual components.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset({'_inputs', 'element_spec'})\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_GeneratorState',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_apply_options',\n",
       " '_as_serialized_graph',\n",
       " '_checkpoint_dependencies',\n",
       " '_component_metadata',\n",
       " '_consumers',\n",
       " '_deferred_dependencies',\n",
       " '_flat_shapes',\n",
       " '_flat_structure',\n",
       " '_flat_types',\n",
       " '_from_components',\n",
       " '_functions',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_graph',\n",
       " '_handle_deferred_dependencies',\n",
       " '_has_captured_ref',\n",
       " '_inputs',\n",
       " '_is_graph_tensor',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_maybe_initialize_trackable',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_no_dependency',\n",
       " '_object_identifier',\n",
       " '_preload_simple_restoration',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_setattr_tracking',\n",
       " '_shape_invariant_to_type_spec',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_to_components',\n",
       " '_trace_variant_creation',\n",
       " '_track_trackable',\n",
       " '_tracking_metadata',\n",
       " '_type_spec',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " '_variant_tensor',\n",
       " 'apply',\n",
       " 'batch',\n",
       " 'cache',\n",
       " 'concatenate',\n",
       " 'element_spec',\n",
       " 'enumerate',\n",
       " 'filter',\n",
       " 'flat_map',\n",
       " 'from_generator',\n",
       " 'from_tensor_slices',\n",
       " 'from_tensors',\n",
       " 'interleave',\n",
       " 'list_files',\n",
       " 'map',\n",
       " 'options',\n",
       " 'padded_batch',\n",
       " 'prefetch',\n",
       " 'range',\n",
       " 'reduce',\n",
       " 'repeat',\n",
       " 'shard',\n",
       " 'shuffle',\n",
       " 'skip',\n",
       " 'take',\n",
       " 'unbatch',\n",
       " 'window',\n",
       " 'with_options',\n",
       " 'zip']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QN_8wwc5R7Qm"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Rco85bbkDfN"
   },
   "source": [
    "## Use the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dvl1uUukc4K"
   },
   "source": [
    "### Shuffle and batch the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTXdRMPcSXZj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w69Jl8k6lilg"
   },
   "source": [
    "### Build and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Input',\n",
       " 'Model',\n",
       " 'Sequential',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_sys',\n",
       " 'activations',\n",
       " 'applications',\n",
       " 'backend',\n",
       " 'callbacks',\n",
       " 'constraints',\n",
       " 'datasets',\n",
       " 'estimator',\n",
       " 'experimental',\n",
       " 'initializers',\n",
       " 'layers',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'mixed_precision',\n",
       " 'models',\n",
       " 'optimizers',\n",
       " 'preprocessing',\n",
       " 'regularizers',\n",
       " 'utils',\n",
       " 'wrappers']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BinaryCrossentropy',\n",
       " 'CategoricalCrossentropy',\n",
       " 'CategoricalHinge',\n",
       " 'CosineSimilarity',\n",
       " 'Hinge',\n",
       " 'Huber',\n",
       " 'KLD',\n",
       " 'KLDivergence',\n",
       " 'LogCosh',\n",
       " 'Loss',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MSE',\n",
       " 'MSLE',\n",
       " 'MeanAbsoluteError',\n",
       " 'MeanAbsolutePercentageError',\n",
       " 'MeanSquaredError',\n",
       " 'MeanSquaredLogarithmicError',\n",
       " 'Poisson',\n",
       " 'Reduction',\n",
       " 'SparseCategoricalCrossentropy',\n",
       " 'SquaredHinge',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'binary_crossentropy',\n",
       " 'categorical_crossentropy',\n",
       " 'categorical_hinge',\n",
       " 'cosine_similarity',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'hinge',\n",
       " 'kld',\n",
       " 'kullback_leibler_divergence',\n",
       " 'logcosh',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_logarithmic_error',\n",
       " 'mse',\n",
       " 'msle',\n",
       " 'poisson',\n",
       " 'serialize',\n",
       " 'sparse_categorical_crossentropy',\n",
       " 'squared_hinge']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'boston_housing',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'fashion_mnist',\n",
       " 'imdb',\n",
       " 'mnist',\n",
       " 'reuters']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uhxr8py4DkDN"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLDzlPGgOHBx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 3.3858 - sparse_categorical_accuracy: 0.8832\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4994 - sparse_categorical_accuracy: 0.9328\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3683 - sparse_categorical_accuracy: 0.9478\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3143 - sparse_categorical_accuracy: 0.9567\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.2721 - sparse_categorical_accuracy: 0.9621\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.2519 - sparse_categorical_accuracy: 0.9660\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.2263 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.2212 - sparse_categorical_accuracy: 0.9714\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 1s 2ms/step - loss: 0.1974 - sparse_categorical_accuracy: 0.9744\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1922 - sparse_categorical_accuracy: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f712e1b9d68>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2q82yN8mmKIE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 0.6706 - sparse_categorical_accuracy: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6705813758916325, 0.9591]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "numpy.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
